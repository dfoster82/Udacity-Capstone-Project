{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Step 1: Scope the Project and Gather Data](#step1)\n",
    "    * [1.1 Determine Scope](#step1.1)\n",
    "    * [1.2 Describe and Gather Data](#step1.2)\n",
    "    \n",
    "    \n",
    "* [Step 2: Explore and Assess the Data](#step2)\n",
    "    * [2.1 Explore the Data](#step2.1)\n",
    "    * [2.2 Clean the Data](#step2.2)\n",
    "    \n",
    "    \n",
    "* [Step 3: Define the Data Model](#step3)\n",
    "    * [3.1 Design the Conceptual Data Model](#step3.1)    \n",
    "    * [3.2 Map Out Data Pipelines](#step3.2)\n",
    "    \n",
    "    \n",
    "* [Step 4: Run Pipelines to Model the Data](#step4)\n",
    "    * [4.1 Create the Data Model](#step4.1)\n",
    "    * [4.2 Run Data Quality Checks](#step4.2)\n",
    "    * [4.3 Provide Data Dictionary](#step4.3)\n",
    "    \n",
    "    \n",
    "* [Step 5: Complete Project Write-Up](#step5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"mariana_fruit_bat.jfif\" alt=\"Juvenile Mariana Fruit Bat\" width=\"300\"/>\n",
    "\n",
    "# Tracking the Vespertilionidae Family of Bats\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "I am interested in biology and zoology, so rather than work on the Udacity-provided data sets, I wanted to research data sets on my own to design a project that held some personal interest. I started by deciding that I wanted to learn more about how bats (specifically those in the Vespertilionidae family) are tracked for scientific studies and then researched data sets that would provide this information as well as more general data about these bats as a species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports and installs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import os\n",
    "import hashlib\n",
    "import csv\n",
    "import json\n",
    "import io\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import configparser\n",
    "from configparser import ConfigParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data<a class=\"anchor\" id=\"step1\"></a>\n",
    "\n",
    "#### 1.1 Determine Scope<a class=\"anchor\" id=\"step1.1\"></a>\n",
    "I used three primary sets of data:\n",
    "\n",
    "* GBIF occurrence data\n",
    "* Movebank data\n",
    "* List of vernacular (common) names\n",
    "\n",
    "\n",
    "I imagined if I were a wildlife biologist or scientist, I might want to be able to quickly determine which species are being studied and tracked. I wanted to gather and combine the data sets I chose to show if any species that are listed in the GBIF occurrence data are included in any tracking studies. I used Amazon S3 to store the data and Amazon Redshift to design the database.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 1.2 Describe and Gather Data<a class=\"anchor\" id=\"step1.2\"></a>\n",
    "I analyzed three primary sets of data:\n",
    "\n",
    "* GBIF occurrence data\n",
    "* Movebank data\n",
    "* List of vernacular (common) names\n",
    "\n",
    "The following text is copied from the GBIF website:\n",
    "\n",
    "\n",
    "> GBIF—the Global Biodiversity Information Facility—is an international network and data infrastructure funded by the world's governments and aimed at providing anyone, anywhere, open access to data about all types of life on Earth.\n",
    "\n",
    "> Coordinated through its Secretariat in Copenhagen, the GBIF network of participating countries and organizations, working through participant nodes, provides data-holding institutions around the world with common standards, best practices and open-source tools enabling them to share information about where and when species have been recorded. This knowledge derives from many sources, including everything from museum specimens collected in the 18th and 19th century to geotagged smartphone photos shared by amateur naturalists in recent days and weeks.\n",
    "\n",
    "I used the GBIF database interface online to find occurrence data for the bats I was interested in. I drilled down through the scientific classifications to the Vespertilionidae family, which is a family of microbats. I then downloaded the resulting CSV file from the website to use in my analysis. This left me with 1,886,467 rows of data. The full list of fields was as follows:\n",
    "* gbifID\n",
    "* datasetKey\n",
    "* occurrenceID\n",
    "* kingdom\n",
    "* phylum\n",
    "* class\n",
    "* order\n",
    "* family\n",
    "* genus\n",
    "* species\n",
    "* infraspecificEpithet\n",
    "* taxonRank\n",
    "* scientificName\n",
    "* verbatimScientificName\n",
    "* verbatimScientificNameAuthorship\n",
    "* countryCode\n",
    "* locality\n",
    "* stateProvince\n",
    "* occurrenceStatus\n",
    "* individualCount\n",
    "* publishingOrgKey\n",
    "* decimalLatitude\n",
    "* decimalLongitude\n",
    "* coordinateUncertaintyInMeters\n",
    "* coordinatePrecision\n",
    "* elevation\n",
    "* elevationAccuracy\n",
    "* depth\n",
    "* depthAccuracy\n",
    "* eventDate\n",
    "* day\n",
    "* month\n",
    "* year\n",
    "* taxonKey\n",
    "* speciesKey\n",
    "* basisOfRecord\n",
    "* institutionCode\n",
    "* collectionCode\n",
    "* catalogNumber\n",
    "* recordNumber\n",
    "* identifiedBy\n",
    "* dateIdentified\n",
    "* license\n",
    "* rightsHolder\n",
    "* recordedBy\n",
    "* typeStatus\n",
    "* establishmentMeans\n",
    "* lastInterpreted\n",
    "* mediaType\n",
    "* issue\n",
    "\n",
    "These fields had varying data types—some were objects, some were floats, some were integers. I knew I would not likely need all of the information, so I wanted to focus on the fields that seemed most relevant to my project.\n",
    "I also used the animal tracking data provided by Movebank (https://www.movebank.org/cms/movebank-main), which is a free online database of animal tracking data hosted by the Max Planck Institute of Animal Behavior.\n",
    "\n",
    "Movebank provides a public REST API for researchers. I followed the detailed instructions provided online, particularly the sample Python code, to gather the data for all studies included in the database. The raw data set contained the following fields:\n",
    "\n",
    "* acknowledgements\n",
    "* citation\n",
    "* go_public_date\n",
    "* grants_used\n",
    "* has_quota\n",
    "* i_am_owner\n",
    "* id\n",
    "* is_test\n",
    "* license_terms\n",
    "* license_type\n",
    "* main_location_lat\n",
    "* main_location_long\n",
    "* name\n",
    "* number_of_deployments\n",
    "* number_of_individuals\n",
    "* number_of_tags\n",
    "* principal_investigator_address\n",
    "* principal_investigator_email\n",
    "* principal_investigator_name\n",
    "* study_objective\n",
    "* study_type\n",
    "* suspend_license_terms\n",
    "* i_can_see_data\n",
    "* there_are_data_which_i_cannot_see\n",
    "* i_have_download_access\n",
    "* i_am_collaborator\n",
    "* study_permission\n",
    "* timestamp_first_deployed_location\n",
    "* timestamp_last_deployed_location\n",
    "* number_of_deployed_locations\n",
    "* taxon_ids\n",
    "* sensor_type_ids\n",
    "* contact_person_name\n",
    "\n",
    "I did change one line of code to be able to view all of the data: I set “true” for the “i_can_see_data” and “there_are_data_which_i_cannot_see” columns.\n",
    "\n",
    "\n",
    "As with the GBIF data, these fields had varying data types—some were objects, some were floats, some were integers—and I reviewed the list of columns and a sample of the data to find the most relevant information.\n",
    "\n",
    "\n",
    "To find the vernacular names of these bat species, I used the Integrated Taxonomic Information System (ITIS) online.\n",
    "\n",
    "\n",
    "The Integrated Taxonomic Information System contains authoritative taxonomic information on plants, animals, fungi, and microbes of North America and the world. It is a partnership of U.S., Canadian, and Mexican agencies (ITIS-North America); other organizations; and taxonomic specialists. ITIS is also a partner of Species 2000 and the Global Biodiversity Information Facility (GBIF). The ITIS and Species 2000 Catalogue of Life (CoL) partnership provides the taxonomic backbone to the Encyclopedia of Life (EOL).\n",
    "\n",
    "\n",
    "\n",
    "I searched for common names containing “bat” in every kingdom. I then copied and pasted the list into an Excel file. I had to do quite a bit of pre-processing for the data in Excel before doing any type of analysis. I formatted the columns, deleted any extraneous information (such as entries for “wombat” or other irrelevant names as well as the researcher name and year), concatenated text as appropriate, formatted the text as proper case, removed duplicates, and cleaned up extra spacing. I also sorted the scientific names chronologically and concatenated any common names if the species had more than one common name listed for that species. I then spell-checked the list, cleaned up punctuation, and fixed typos (one entry had “trumped-eared bat” instead of “trumpet-eared bat”). I saved the Excel file as a CSV file to use in the Jupyter notebook.\n",
    "\n",
    "After analyzing these data sets to get a better idea of which columns I might need and what format they were in, I started developing a preliminary data model using a star schema. I decided to use Amazon S3 and Redshift to store and analyze the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in GBIF file\n",
    "\n",
    "df_gbif = pd.read_csv('GBIF_Bat_Occurrences.csv', sep='\\t', error_bad_lines=False)\n",
    "\n",
    "df_gbif.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_gbif.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_gbif.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in Vernacular Names file\n",
    "\n",
    "df_vern_name = pd.read_csv('Bats_Vernacular_Names.csv', encoding = 'unicode_escape')\n",
    "\n",
    "df_vern_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_vern_name.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_vern_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import Movebank data\n",
    "\n",
    "def callMovebankAPI(params):\n",
    "    # Requests Movebank API with ((param1, value1), (param2, value2),).\n",
    "    # Assumes the environment variables 'mbus' (Movebank user name) and 'mbpw' (Movebank password).\n",
    "    # Returns the API response as plain text.\n",
    "\n",
    "    parser = configparser.ConfigParser()\n",
    "    parser.read('movebank_config.ini')\n",
    "    movebank_usrnm = parser.get('movebank_auth', 'movebank_username')\n",
    "    movebank_pwd = parser.get('movebank_auth', 'movebank_password')\n",
    "\n",
    "    response = requests.get('https://www.movebank.org/movebank/service/direct-read', params=params,\n",
    "                            auth=HTTPBasicAuth(movebank_usrnm, movebank_pwd))\n",
    "    print(\"Request \" + response.url)\n",
    "    if response.status_code == 200:  # successful request\n",
    "        if 'License Terms:' in str(response.content):\n",
    "            # only the license terms are returned, hash and append them in a subsequent request.\n",
    "            # See also\n",
    "            # https://github.com/movebank/movebank-api-doc/blob/master/movebank-api.md#read-and-accept-license-terms-using-curl\n",
    "            print(\"Has license terms\")\n",
    "            hash = hashlib.md5(response.content).hexdigest()\n",
    "            params = params + (('license-md5', hash),)\n",
    "            # also attach previous cookie:\n",
    "            response = requests.get('https://www.movebank.org/movebank/service/direct-read', params=params,\n",
    "                                    cookies=response.cookies, auth=HTTPBasicAuth(movebank_usrnm, movebank_pwd))\n",
    "            if response.status_code == 403:  # incorrect hash\n",
    "                print(\"Incorrect hash\")\n",
    "                return ''\n",
    "        return response.content.decode('utf-8')\n",
    "    print(str(response.content))\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getStudies():\n",
    "    studies = callMovebankAPI((('entity_type', 'study'), ('i_can_see_data', 'true'), ('there_are_data_which_i_cannot_see', 'true')))\n",
    "    if len(studies) > 0:\n",
    "        # parse raw text to dicts\n",
    "        studies = csv.DictReader(io.StringIO(studies), delimiter=',')\n",
    "        return [s for s in studies if s['i_can_see_data'] == 'true' or s['there_are_data_which_i_cannot_see'] == 'true']\n",
    "    return []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    allstudies = getStudies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_movebank_all = pd.DataFrame.from_records(allstudies)\n",
    "\n",
    "df_movebank_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_movebank_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_movebank_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_movebank_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_movebank_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data<a class=\"anchor\" id=\"step2\"></a>\n",
    "#### 2.1 Explore the Data<a class=\"anchor\" id=\"step2.1\"></a>\n",
    "To clean the data, I identified how the pandas dataframes would need to be combined or split to create the final staging table, identified any missing values, identified any duplicates, and identified and fixed any data types that were not compatible with the proposed data model.\n",
    "\n",
    "#### 2.2 Clean the Data<a class=\"anchor\" id=\"step2.2\"></a>\n",
    "The following list briefly outlines the steps I followed to clean the data:\n",
    "\n",
    "* Split Movebank taxon ids into multiple rows (these were originally in one row separated by commas)\n",
    "* Created a new pandas dataframe with only relevant columns for the Movebank data\n",
    "* Merged the two dataframes\n",
    "* Created a new dataframe with only relevant columns for the GBIF data\n",
    "* Concatenated the Movebank and GBIF dataframes to create the main staging table\n",
    "* Merged main staging dataframe and vernacular names dataframe to add a column for the vernacular names\n",
    "* Checked for missing values (any missing values were converted to 0 [fill N/A with 0])\n",
    "* Checked for duplicates (none)\n",
    "* Fixed incorrect data types\n",
    "* Changed gbifID to int \n",
    "* Changed taxonKey to int\n",
    "* Changed id to int\n",
    "* Changed day, month, and year to int\n",
    "* Changed eventDate to date\n",
    "* Reviewed list of columns and head for final staging table dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_clean_up(df1, df2, df3):\n",
    "    \n",
    "    # Explode/split taxon IDs into multiple rows\n",
    "    df_movebank_split = pd.DataFrame(df1['taxon_ids'].str.split(',').tolist(), index=df1['id']).stack()\n",
    "    df_movebank_split = df_movebank_split.reset_index([0, 'id'])\n",
    "    df_movebank_split.columns = ['id', 'taxon_ids']\n",
    "    \n",
    "    # Create new dataframe with only needed columns\n",
    "    df_movebank_subset = df1[['id', 'name', 'study_objective', 'study_type', 'citation', 'taxon_ids']]\n",
    "    \n",
    "    # Create new dataframe to merge two dataframes so taxon IDs are split\n",
    "    df1 = pd.merge(df_movebank_split, df_movebank_subset, how='left', on=['id', 'taxon_ids'])\n",
    "    \n",
    "    # Create new dataframe with only needed columns for GBIF dataframe\n",
    "    df_gbif_subset = df2[['taxonKey', 'species', 'gbifID', 'stateProvince', 'countryCode', 'decimalLatitude',\n",
    "                          'decimalLongitude', 'eventDate', 'day', 'month', 'year']]\n",
    "    \n",
    "    # Concatenate Movebank and GBIF dataframes to create main staging table\n",
    "\n",
    "    # Rename column in Movebank dataframe\n",
    "    df1 = df1.rename(columns={'taxon_ids': 'species'})\n",
    "    \n",
    "    # Concatenate Movebank and GBIF dataframes\n",
    "    df_main_staging = pd.concat(\n",
    "                      [df_gbif_subset, df1],\n",
    "                      join='outer',\n",
    "                      ignore_index=True,\n",
    "                      verify_integrity=True)\n",
    "    \n",
    "    # Add column for vernacular names\n",
    "    \n",
    "    # Rename columns\n",
    "    df3 = df3.rename(columns={'speciesName': 'species'})\n",
    "    \n",
    "    # Merge dataframes\n",
    "    df_main_staging_complete = pd.merge(df_main_staging, df3,\n",
    "                                        how='outer',\n",
    "                                        on='species')\n",
    "    \n",
    "    # Check for missing values\n",
    "    df_main_staging_complete.isnull().sum\n",
    "    \n",
    "    # Check for duplicates\n",
    "    df_main_staging_complete.duplicated()\n",
    "    df_main_staging_complete.loc[df_main_staging_complete.duplicated(), :]\n",
    "    \n",
    "    # Fill missing values with 0 and change data type to int\n",
    "    df_main_staging_complete['gbifID'] = df_main_staging_complete['gbifID'].fillna(0).astype(int)\n",
    "    df_main_staging_complete['taxonKey'] = df_main_staging_complete['taxonKey'].fillna(0).astype(int)\n",
    "    df_main_staging_complete['id'] = df_main_staging_complete['id'].fillna(0).astype(int)\n",
    "    df_main_staging_complete['day'] = df_main_staging_complete['day'].fillna(0).astype(int)\n",
    "    df_main_staging_complete['month'] = df_main_staging_complete['month'].fillna(0).astype(int)\n",
    "    df_main_staging_complete['year'] = df_main_staging_complete['year'].fillna(0).astype(int)\n",
    "    \n",
    "    # Change eventDate to date\n",
    "    df_main_staging_complete['eventDate'] = pd.to_datetime(df_main_staging_complete['eventDate']).dt.date\n",
    "    \n",
    "    return df_main_staging_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_main_staging_complete = process_clean_up(df_movebank_all, df_gbif, df_vern_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_main_staging_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model<a class=\"anchor\" id=\"step3\"></a>\n",
    "#### 3.1 Design the Conceptual Data Model<a class=\"anchor\" id=\"step3.1\"></a>\n",
    "The image below (created in Lucidchart) shows the conceptual data model. I chose to use a star schema because it is simple (no need for complex joins), it is optimized for reading data more quickly, and it uses simpler business logic to communicate to stakeholders and other database users.\n",
    "\n",
    "#### 3.2 Map Out Data Pipelines<a class=\"anchor\" id=\"step3.2\"></a>\n",
    "The following steps were used to pipeline the data into the chosen data model:\n",
    "* Convert pandas dataframe to .csv file\n",
    "* Save .csv file in S3 bucket\n",
    "* Create new tables in Redshift database\n",
    "* Create staging table\n",
    "* Load data into final fact and dimension tables in Redshift\n",
    "\n",
    "<img src=\"UdacityFinalProject_DataModel.png\" alt=\"Data Model\" width=\"1200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save new dataframe as .csv file\n",
    "\n",
    "df_main_staging_complete.to_csv('staging_complete.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Upload .csv file to S3 bucket\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "s3 = boto3.client('s3', aws_access_key_id = config.get('S3', 'ACCESS_KEY'),\n",
    "                  aws_secret_access_key = config.get('S3', 'SECRET_KEY'))\n",
    "\n",
    "s3.upload_file('staging_complete.csv', 'udacityfinalprojectanimaltracking', 'staging_complete.csv')\n",
    "\n",
    "print('Upload successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data<a class=\"anchor\" id=\"step4\"></a>\n",
    "#### 4.1 Create the Data Model<a class=\"anchor\" id=\"step4.1\"></a>\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create tables — separate file\n",
    "\n",
    "%run create_tables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Run ETL data pipeline — separate file\n",
    "\n",
    "%run etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Run Data Quality Checks<a class=\"anchor\" id=\"step4.2\"></a>\n",
    "I designated distinct data types for all of the columns used in the database tables. I also frequently checked the number of rows for the resultant data using the pandas .shape property and also previewed the data using the pandas .head()  function. Once the data was loaded in Redshift, I previewed the schema for each table and ran SQL queries to perform count checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Provide Data Dictionary<a class=\"anchor\" id=\"step4.3\"></a>\n",
    "Please see the separate file for the data dictionary: UdacityFinalProject_DataDictionary.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write-Up<a class=\"anchor\" id=\"step5\"></a>\n",
    "\n",
    "The ETL successfully processed the result in the final data model as shown in the following screenshots.\n",
    "\n",
    "The final fact table (factBatTracking) has 1,893,739 rows.\n",
    "\n",
    "<img src=\"fact_total_count_query.png\" width=\"300\"/>\n",
    "<img src=\"fact_total_count_result.png\" width=\"300\"/>\n",
    "\n",
    "The following screenshots show a sample query to find the number of occurrences for each year after the year 2000.\n",
    "\n",
    "<img src=\"year_query.png\" width=\"500\"/>\n",
    "<img src=\"year_result.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "The sample query below shows results for any studies associated with fruit bats listed in the fact table.\n",
    "\n",
    "\n",
    "<img src=\"fruit_bat_query.png\" width=\"500\"/>\n",
    "<img src=\"fruit_bat_result.png\" width=\"500\"/>\n",
    "\n",
    "\n",
    "For this project, I chose to use an Amazon S3 bucket to store data files and Amazon Redshift as a database platform because I felt relatively comfortable using these tools and these seemed to fit my goals for this project (creating a cloud data warehouse using a star schema for analytics).\n",
    "    \n",
    "For this type of project, the data should likely be updated every week. This would provide users with enough up-to-date information for analysis. Every day would be overkill; every month would not be often enough.\n",
    " \n",
    "*If the data were increased by 100×*, I would likely use a different platform such as Spark to work with such a huge amount of data, possibly using a combination of Spark and Apache Livy in an EMR cluster.\n",
    "\n",
    "*If the data populated a dashboard that had to be updated on a daily basis by 7 a.m. every day*, I would probably use a workflow management platform such as Airflow to schedule and monitor data workflows. I would also need to create a script to update the GBIF CSV file or connect to the GBIF API.\n",
    "\n",
    "*If the database needed to be accessed by 100+ people*, I would consider using an enterprise data architecture, such as MySQL or Oracle, but Redshift would likely still be a good choice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab1e8248fda49f3c9e00627f49e16ed4394d793a7c3810507e825c48c90d066e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
